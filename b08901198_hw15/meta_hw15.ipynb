{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "meta_hw15.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "47e3c1d635924a6f8bf421af30d3a750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e8e87a326ac42908b233a8d2a4e724f",
              "IPY_MODEL_37c1a03113944a62b282c3ffb671668e",
              "IPY_MODEL_b2549fe174eb41bc9a9c8287184503f0"
            ],
            "layout": "IPY_MODEL_0a7d50239c0a4d12a8e5ac7a39f80196"
          }
        },
        "9e8e87a326ac42908b233a8d2a4e724f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aa6c9811d6245cb945ebc0bba9de744",
            "placeholder": "​",
            "style": "IPY_MODEL_260d58e6e4bf488fb1f8e2a211504060",
            "value": "100%"
          }
        },
        "37c1a03113944a62b282c3ffb671668e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7fb7ffef19f4506a79ab4dbcc88b7a5",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_405604dbf5eb4bf5a9fc492d9c1a3113",
            "value": 32
          }
        },
        "b2549fe174eb41bc9a9c8287184503f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63f22a0761f044b8baf229d10925a4af",
            "placeholder": "​",
            "style": "IPY_MODEL_136db2916bb5419cb898ad9c02c02959",
            "value": " 32/32 [01:52&lt;00:00,  3.50s/it]"
          }
        },
        "0a7d50239c0a4d12a8e5ac7a39f80196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aa6c9811d6245cb945ebc0bba9de744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "260d58e6e4bf488fb1f8e2a211504060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7fb7ffef19f4506a79ab4dbcc88b7a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "405604dbf5eb4bf5a9fc492d9c1a3113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63f22a0761f044b8baf229d10925a4af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136db2916bb5419cb898ad9c02c02959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzVBe3h7Xh-2"
      },
      "source": [
        "<a name=\"top\"></a>\n",
        "# **HW15 Meta Learning: Few-shot Classification**\n",
        "\n",
        "This is the sample code for homework 15.\n",
        "\n",
        "Please mail to mlta-2022-spring@googlegroups.com if you have any questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdpzIMG6XsGK"
      },
      "source": [
        "## **Step 0: Check GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjjHsZbaL7SV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c35d8e-24ad-477d-9cbc-4bf7111f26a2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 29 12:44:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ3wvyjnXwGX"
      },
      "source": [
        "## **Step 1: Download Data**\n",
        "\n",
        "Run the cell to download data, which has been pre-processed by TAs.  \n",
        "The dataset has been augmented, so extra data augmentation is not required.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Gt4Jucug41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1303a7b2-dd44-4cbe-e0fa-daa55b9cb9ab"
      },
      "source": [
        "workspace_dir = '.'\n",
        "\n",
        "# Download dataset\n",
        "!wget https://github.com/xraychen/shiny-disco/releases/download/Latest/omniglot.tar.gz \\\n",
        "    -O \"{workspace_dir}/Omniglot.tar.gz\"\n",
        "!wget https://github.com/xraychen/shiny-disco/releases/download/Latest/omniglot-test.tar.gz \\\n",
        "    -O \"{workspace_dir}/Omniglot-test.tar.gz\"\n",
        "\n",
        "# Use `tar' command to decompress\n",
        "!tar -zxf \"{workspace_dir}/Omniglot.tar.gz\" -C \"{workspace_dir}/\"\n",
        "!tar -zxf \"{workspace_dir}/Omniglot-test.tar.gz\" -C \"{workspace_dir}/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-29 12:44:56--  https://github.com/xraychen/shiny-disco/releases/download/Latest/omniglot.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/497831409/3fe2424d-01cd-4118-b05d-bbd9395e720c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220629%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220629T124456Z&X-Amz-Expires=300&X-Amz-Signature=10de18008c462b164f434441e84362d142ddc5b7c66346d864fff1d3dee8fc1f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=497831409&response-content-disposition=attachment%3B%20filename%3Domniglot.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-29 12:44:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/497831409/3fe2424d-01cd-4118-b05d-bbd9395e720c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220629%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220629T124456Z&X-Amz-Expires=300&X-Amz-Signature=10de18008c462b164f434441e84362d142ddc5b7c66346d864fff1d3dee8fc1f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=497831409&response-content-disposition=attachment%3B%20filename%3Domniglot.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5718170 (5.5M) [application/octet-stream]\n",
            "Saving to: ‘./Omniglot.tar.gz’\n",
            "\n",
            "./Omniglot.tar.gz   100%[===================>]   5.45M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-06-29 12:44:56 (170 MB/s) - ‘./Omniglot.tar.gz’ saved [5718170/5718170]\n",
            "\n",
            "--2022-06-29 12:44:56--  https://github.com/xraychen/shiny-disco/releases/download/Latest/omniglot-test.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/497831409/b220f300-aa97-4b00-a2a9-99b3bc49b322?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220629%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220629T124456Z&X-Amz-Expires=300&X-Amz-Signature=3281d361718a58bd9ff29a9c081e8057d4814e616785d322c86070dd21349094&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=497831409&response-content-disposition=attachment%3B%20filename%3Domniglot-test.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-29 12:44:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/497831409/b220f300-aa97-4b00-a2a9-99b3bc49b322?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220629%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220629T124456Z&X-Amz-Expires=300&X-Amz-Signature=3281d361718a58bd9ff29a9c081e8057d4814e616785d322c86070dd21349094&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=497831409&response-content-disposition=attachment%3B%20filename%3Domniglot-test.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1838141 (1.8M) [application/octet-stream]\n",
            "Saving to: ‘./Omniglot-test.tar.gz’\n",
            "\n",
            "./Omniglot-test.tar 100%[===================>]   1.75M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-06-29 12:44:57 (180 MB/s) - ‘./Omniglot-test.tar.gz’ saved [1838141/1838141]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baVsWfcSYHVN"
      },
      "source": [
        "## **Step 2: Build the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqiOdDLgYOlQ"
      },
      "source": [
        "### Library importation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9pfkqh8gxHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4f2bac-cf5e-496c-fd7c-97f6fc4ce401"
      },
      "source": [
        "# Import modules we need\n",
        "import glob, random\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Check device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"DEVICE = {device}\")\n",
        "\n",
        "# Fix random seeds\n",
        "random_seed = 0\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(random_seed)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE = cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tnBcu4PfiXpa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TlwLtC1YRT7"
      },
      "source": [
        "### Model Construction Preliminaries\n",
        "\n",
        "Since our task is image classification, we need to build a CNN-based model.  \n",
        "However, to implement MAML algorithm, we should adjust some code in `nn.Module`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFwB3tuEDYfy"
      },
      "source": [
        "Take a look at MAML pseudocode...\n",
        "\n",
        "<img src=\"https://i.imgur.com/9aHlvfX.png\" width=\"50%\" />\n",
        "\n",
        "On the 10-th line, what we take gradients on are those $\\theta$ representing  \n",
        "<font color=\"#0CC\">**the original model parameters**</font> (outer loop) instead of those in  the  \n",
        "<font color=\"#0C0\">**inner loop**</font>, so we need to use `functional_forward` to compute the output  \n",
        "logits of input image instead of `forward` in `nn.Module`.\n",
        "\n",
        "The following defines these functions.\n",
        "\n",
        "<!-- 由於在第10行，我們是要對原本的參數 θ 微分，並非 inner-loop (Line5~8) 的 θ' 微分，因此在 inner-loop，我們需要用 functional forward 的方式算出 input image 的 output logits，而不是直接用 nn.module 裡面的 forward（直接對 θ 微分）。在下面我們分別定義了 functional forward 以及 forward 函數。 -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuYQiPeQYc__"
      },
      "source": [
        "### Model block definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgFbbKHYg3Hk"
      },
      "source": [
        "def ConvBlock(in_ch: int, out_ch: int):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "\n",
        "\n",
        "def ConvBlockFunction(x, w, b, w_bn, b_bn):\n",
        "    x = F.conv2d(x, w, b, padding=1)\n",
        "    x = F.batch_norm(\n",
        "        x, running_mean=None, running_var=None, weight=w_bn, bias=b_bn, training=True\n",
        "    )\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQEzgWN7fi7B"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bFBGEQoHQUW"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_ch, k_way):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.conv1 = ConvBlock(in_ch, 128)\n",
        "        self.conv2 = ConvBlock(128, 256)\n",
        "        self.conv3 = ConvBlock(256, 128)\n",
        "        self.conv4 = ConvBlock(128, 64)\n",
        "        self.logits = nn.Linear(64, k_way)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)+x\n",
        "        x = self.conv3(x)+x\n",
        "        x = self.conv4(x)+x\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.logits(x)\n",
        "        return x\n",
        "\n",
        "    def functional_forward(self, x, params):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        x: input images [batch, 1, 28, 28]\n",
        "        params: model parameters,\n",
        "                i.e. weights and biases of convolution\n",
        "                     and weights and biases of\n",
        "                                   batch normalization\n",
        "                type is an OrderedDict\n",
        "\n",
        "        Arguments:\n",
        "        x: input images [batch, 1, 28, 28]\n",
        "        params: The model parameters,\n",
        "                i.e. weights and biases of convolution\n",
        "                     and batch normalization layers\n",
        "                It's an `OrderedDict`\n",
        "        \"\"\"\n",
        "        for block in [1, 2, 3, 4]:\n",
        "            x = ConvBlockFunction(\n",
        "                x,\n",
        "                params[f\"conv{block}.0.weight\"],\n",
        "                params[f\"conv{block}.0.bias\"],\n",
        "                params.get(f\"conv{block}.1.weight\"),\n",
        "                params.get(f\"conv{block}.1.bias\"),\n",
        "            )\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.linear(x, params[\"logits.weight\"], params[\"logits.bias\"])\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmJq_0B9Yj0G"
      },
      "source": [
        "### Create Label\n",
        "\n",
        "This function is used to create labels.  \n",
        "In a N-way K-shot few-shot classification problem,\n",
        "each task has `n_way` classes, while there are `k_shot` images for each class.  \n",
        "This is a function that creates such labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQF5vgLvg5aX",
        "outputId": "88d76bd6-2c45-4484-963f-03295cec16a3"
      },
      "source": [
        "def create_label(n_way, k_shot):\n",
        "    return torch.arange(n_way).repeat_interleave(k_shot).long()\n",
        "\n",
        "\n",
        "# Try to create labels for 5-way 2-shot setting\n",
        "create_label(5, 2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nCFv9PGw50J"
      },
      "source": [
        "### Accuracy calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FahDr0xQw50S"
      },
      "source": [
        "def calculate_accuracy(logits, labels):\n",
        "    \"\"\"utility function for accuracy calculation\"\"\"\n",
        "    acc = np.asarray(\n",
        "        [(torch.argmax(logits, -1).cpu().numpy() == labels.cpu().numpy())]\n",
        "    ).mean()\n",
        "    return acc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hl7ro2mYzsI"
      },
      "source": [
        "### Define Dataset\n",
        "\n",
        "Define the dataset.  \n",
        "The dataset returns images of a random character, with (`k_shot + q_query`) images,  \n",
        "so the size of returned tensor is `[k_shot+q_query, 1, 28, 28]`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tJ2mot9hHPb"
      },
      "source": [
        "# Dataset for train and val\n",
        "class Omniglot(Dataset):\n",
        "    def __init__(self, data_dir, k_way, q_query, task_num=None):\n",
        "        self.file_list = [\n",
        "            f for f in glob.glob(data_dir + \"**/character*\", recursive=True)\n",
        "        ]\n",
        "        # limit task number if task_num is set\n",
        "        if task_num is not None:\n",
        "            self.file_list = self.file_list[: min(len(self.file_list), task_num)]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.n = k_way + q_query\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = np.arange(20)\n",
        "\n",
        "        # For random sampling the characters we want.\n",
        "        np.random.shuffle(sample)\n",
        "        img_path = self.file_list[idx]\n",
        "        img_list = [f for f in glob.glob(img_path + \"**/*.png\", recursive=True)]\n",
        "        img_list.sort()\n",
        "        imgs = [self.transform(Image.open(img_file)) for img_file in img_list]\n",
        "        # `k_way + q_query` examples for each character\n",
        "        imgs = torch.stack(imgs)[sample[: self.n]]\n",
        "        return imgs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Omniglot2(Dataset):\n",
        "    def __init__(self, data_dir, k_way, q_query, i,task_num=None):\n",
        "        self.file_list = [\n",
        "            f for f in glob.glob(data_dir + \"**/character*\", recursive=True)\n",
        "        ]\n",
        "        # limit task number if task_num is set\n",
        "        if task_num is not None:\n",
        "            self.file_list = self.file_list[: min(len(self.file_list), task_num)]\n",
        "        self.transform = transforms.Compose([transforms.RandomRotation(i*22.5),transforms.ToTensor()])\n",
        "        self.n = k_way + q_query\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = np.arange(20)\n",
        "\n",
        "        # For random sampling the characters we want.\n",
        "        np.random.shuffle(sample)\n",
        "        img_path = self.file_list[idx]\n",
        "        img_list = [f for f in glob.glob(img_path + \"**/*.png\", recursive=True)]\n",
        "        img_list.sort()\n",
        "        imgs = [self.transform(Image.open(img_file)) for img_file in img_list]\n",
        "        # `k_way + q_query` examples for each character\n",
        "        imgs = torch.stack(imgs)[sample[: self.n]]\n",
        "        return imgs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)"
      ],
      "metadata": {
        "id": "feV-6QDY0auv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Learning Algorithms**\n",
        "\n",
        "### Transfer learning\n",
        "\n",
        "The solver first chose five task from the training set, then do normal classification training on the chosen five tasks. In inference, the model finetune for `inner_train_step` steps on the support set images, and than do inference on the query set images.\n",
        "\n",
        "For consistant with the meta-learning solver, the base solver has the exactly same input and output format with the meta-learning solver.\n",
        "\n"
      ],
      "metadata": {
        "id": "WRzjBWhwI6tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BaseSolver(\n",
        "    model,\n",
        "    optimizer,\n",
        "    x,\n",
        "    n_way,\n",
        "    k_shot,\n",
        "    q_query,\n",
        "    loss_fn,\n",
        "    inner_train_step=1,\n",
        "    inner_lr=0.4,\n",
        "    train=True,\n",
        "    return_labels=False,\n",
        "):\n",
        "    criterion, task_loss, task_acc = loss_fn, [], []\n",
        "    labels = []\n",
        "\n",
        "    for meta_batch in x:\n",
        "        # Get data\n",
        "        support_set = meta_batch[: n_way * k_shot]\n",
        "        query_set = meta_batch[n_way * k_shot :]\n",
        "        \n",
        "\n",
        "        if train:\n",
        "            \"\"\" training loop \"\"\"\n",
        "            # Use the support set to calculate loss\n",
        "            labels = create_label(n_way, k_shot).to(device)\n",
        "            logits = model.forward(support_set)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            task_loss.append(loss)\n",
        "            task_acc.append(calculate_accuracy(logits, labels))\n",
        "        else:\n",
        "            \"\"\" validation / testing loop \"\"\"\n",
        "            # First update model with support set images for `inner_train_step` steps\n",
        "            fast_weights = OrderedDict(model.named_parameters())\n",
        "\n",
        "\n",
        "            for inner_step in range(inner_train_step):\n",
        "                # Simply training\n",
        "                train_label = create_label(n_way, k_shot).to(device)\n",
        "                logits = model.functional_forward(support_set, fast_weights)\n",
        "                loss = criterion(logits, train_label)\n",
        "\n",
        "                grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)\n",
        "                # Perform SGD\n",
        "                fast_weights = OrderedDict(\n",
        "                    (name, param - inner_lr * grad)\n",
        "                    for ((name, param), grad) in zip(fast_weights.items(), grads)\n",
        "                )\n",
        "\n",
        "            if not return_labels:\n",
        "                \"\"\" validation \"\"\"\n",
        "                val_label = create_label(n_way, q_query).to(device)\n",
        "\n",
        "                logits = model.functional_forward(query_set, fast_weights)\n",
        "                loss = criterion(logits, val_label)\n",
        "                task_loss.append(loss)\n",
        "                task_acc.append(calculate_accuracy(logits, val_label))\n",
        "            else:\n",
        "                \"\"\" testing \"\"\"\n",
        "                logits = model.functional_forward(query_set, fast_weights)\n",
        "                labels.extend(torch.argmax(logits, -1).cpu().numpy())\n",
        "\n",
        "    if return_labels:\n",
        "        return labels\n",
        "\n",
        "    batch_loss = torch.stack(task_loss).mean()\n",
        "    task_acc = np.mean(task_acc)\n",
        "\n",
        "    if train:\n",
        "        # Update model\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return batch_loss, task_acc"
      ],
      "metadata": {
        "id": "R_jGPJHK7KpC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm5iVp90Ylii"
      },
      "source": [
        "### Meta Learning\n",
        "\n",
        "Here is the main Meta Learning algorithm.\n",
        "\n",
        "Please finish the TODO blocks for the inner and outer loop update rules.\n",
        "\n",
        "- For implementing FO-MAML you can refer to [p.25 of the slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf#page=25&view=FitW).\n",
        "\n",
        "- For the original MAML, you can refer to [the slides of meta learning (p.13 ~ p.18)](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf#page=13&view=FitW).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjNxrWW_yNck"
      },
      "source": [
        "def MetaSolver(\n",
        "    model,\n",
        "    optimizer,\n",
        "    x,\n",
        "    n_way,\n",
        "    k_shot,\n",
        "    q_query,\n",
        "    loss_fn,\n",
        "    inner_train_step=1,\n",
        "    inner_lr=0.4,\n",
        "    train=True,\n",
        "    return_labels=False\n",
        "):\n",
        "    criterion, task_loss, task_acc = loss_fn, [], []\n",
        "    labels = []\n",
        "    for meta_batch in x:\n",
        "        # Get data\n",
        "        support_set = meta_batch[: n_way * k_shot]\n",
        "        query_set = meta_batch[n_way * k_shot :]\n",
        "\n",
        "        # Copy the params for inner loop\n",
        "        fast_weights = OrderedDict(model.named_parameters())\n",
        "\n",
        "        ### ---------- INNER TRAIN LOOP ---------- ###\n",
        "        for inner_step in range(inner_train_step):\n",
        "            # Simply training\n",
        "            train_label = create_label(n_way, k_shot).to(device)\n",
        "            logits = model.functional_forward(support_set, fast_weights)\n",
        "            loss = criterion(logits, train_label)\n",
        "            # Inner gradients update! vvvvvvvvvvvvvvvvvvvv #\n",
        "            \"\"\" Inner Loop Update \"\"\"\n",
        "            # TODO: Finish the inner loop update rule\n",
        "            grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)\n",
        "    # Perform SGD\n",
        "            fast_weights = OrderedDict((name, param - inner_lr * grad) for ((name, param), grad) in zip(fast_weights.items(), grads))\n",
        "            # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ #\n",
        "\n",
        "        ### ---------- INNER VALID LOOP ---------- ###\n",
        "        if not return_labels:\n",
        "            \"\"\" training / validation \"\"\"\n",
        "            val_label = create_label(n_way, q_query).to(device)\n",
        "\n",
        "            # Collect gradients for outer loop\n",
        "            logits = model.functional_forward(query_set, fast_weights)\n",
        "            loss = criterion(logits, val_label)\n",
        "            task_loss.append(loss)\n",
        "            task_acc.append(calculate_accuracy(logits, val_label))\n",
        "        else:\n",
        "            \"\"\" testing \"\"\"\n",
        "            logits = model.functional_forward(query_set, fast_weights)\n",
        "            labels.extend(torch.argmax(logits, -1).cpu().numpy())\n",
        "\n",
        "    if return_labels:\n",
        "        return labels\n",
        "\n",
        "    # Update outer loop\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    meta_batch_loss = torch.stack(task_loss).mean()\n",
        "    if train:\n",
        "        \"\"\" Outer Loop Update \"\"\"\n",
        "        # TODO: Finish the outer loop update\n",
        "        meta_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        #raise NotimplementedError\n",
        "\n",
        "    task_acc = np.mean(task_acc)\n",
        "    return meta_batch_loss, task_acc"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBoRBhVlZAST"
      },
      "source": [
        "## **Step 4: Initialization**\n",
        "\n",
        "After defining all components we need, the following initialize a model before training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-i7aseftUF"
      },
      "source": [
        "### Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wFHmVcBhE4M"
      },
      "source": [
        "n_way = 5\n",
        "k_shot = 1\n",
        "q_query = 1\n",
        "train_inner_train_step = 1\n",
        "val_inner_train_step = 3\n",
        "inner_lr = 0.4\n",
        "meta_lr = 0.001\n",
        "meta_batch_size = 32\n",
        "max_epoch = 130\n",
        "eval_batches = 20\n",
        "train_data_path = \"./Omniglot/images_background/\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvzo7NVpfu5V"
      },
      "source": [
        "### Dataloader initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I13GJavhP0_"
      },
      "source": [
        "def dataloader_init(datasets, shuffle=True, num_workers=2):\n",
        "    train_set, val_set = datasets\n",
        "    train_loader = DataLoader(\n",
        "        train_set,\n",
        "        # The \"batch_size\" here is not \\\n",
        "        #    the meta batch size, but  \\\n",
        "        #    how many different        \\\n",
        "        #    characters in a task,     \\\n",
        "        #    i.e. the \"n_way\" in       \\\n",
        "        #    few-shot classification.\n",
        "        batch_size=n_way,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_set, batch_size=n_way, num_workers=num_workers, shuffle=shuffle, drop_last=True\n",
        "    )\n",
        "\n",
        "    train_iter = iter(train_loader)\n",
        "    val_iter = iter(val_loader)\n",
        "    return (train_loader, val_loader), (train_iter, val_iter)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVund--bfw0e"
      },
      "source": [
        "### Model & optimizer initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxug882ihF2B"
      },
      "source": [
        "def model_init():\n",
        "    meta_model = Classifier(1, n_way).to(device)\n",
        "    optimizer = torch.optim.AdamW(meta_model.parameters(), lr=meta_lr,weight_decay=2e-5)\n",
        "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    return meta_model, optimizer, loss_fn"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj8cLRNLf2zg"
      },
      "source": [
        "### Utility function to get a meta-batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkCSsxOhC-N"
      },
      "source": [
        "def get_meta_batch(meta_batch_size, k_shot, q_query, data_loader, iterator):\n",
        "    data = []\n",
        "    for _ in range(meta_batch_size):\n",
        "        try:\n",
        "            # a \"task_data\" tensor is representing \\\n",
        "            #     the data of a task, with size of \\\n",
        "            #     [n_way, k_shot+q_query, 1, 28, 28]\n",
        "            task_data = iterator.next()\n",
        "        except StopIteration:\n",
        "            iterator = iter(data_loader)\n",
        "            task_data = iterator.next()\n",
        "        train_data = task_data[:, :k_shot].reshape(-1, 1, 28, 28)\n",
        "        val_data = task_data[:, k_shot:].reshape(-1, 1, 28, 28)\n",
        "        task_data = torch.cat((train_data, val_data), 0)\n",
        "        data.append(task_data)\n",
        "    return torch.stack(data).to(device), iterator"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWQczA3FwjEG"
      },
      "source": [
        "<a name=\"mainprog\" id=\"mainprog\"></a>\n",
        "## **Step 5: Main program for training & testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EirEnaof7ep"
      },
      "source": [
        "### Start training!\n",
        "With `solver = 'base'`, the solver is a transfer learning algorithm.\n",
        "\n",
        "Once you finish the TODO blocks in the `MetaSolver`, change the variable `solver = 'meta'` to start training with meta learning algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZjJrLAhBWw"
      },
      "source": [
        "solver = 'meta' # base, meta\n",
        "meta_model, optimizer, loss_fn = model_init()\n",
        "#meta_model.load_state_dict(torch.load(\"/content/meta_best (4).ckpt\"))\n",
        "\n",
        "# init solver and dataset according to solver type\n",
        "if solver == 'base':\n",
        "    max_epoch = 5 # the base solver only needs 5 epochs\n",
        "    Solver = BaseSolver\n",
        "    train_set, val_set = torch.utils.data.random_split(\n",
        "        Omniglot(train_data_path, k_shot, q_query, task_num=10), [5, 5]\n",
        "    )\n",
        "    (train_loader, val_loader), (train_iter, val_iter) = dataloader_init((train_set, val_set), shuffle=False)\n",
        "\n",
        "elif solver == 'meta':\n",
        "    Solver = MetaSolver\n",
        "    dataset =  torch.utils.data.ConcatDataset([Omniglot2(train_data_path, k_shot, q_query,i) for i in range(16)])\n",
        "    train_split = int(0.8 * len(dataset))\n",
        "    val_split = len(dataset) - train_split\n",
        "    train_set, val_set = torch.utils.data.random_split(\n",
        "        dataset, [train_split, val_split]\n",
        "    )\n",
        "    (train_loader, val_loader), (train_iter, val_iter) = dataloader_init((train_set, val_set))\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# main training loop\n",
        "val_acc_best = 0\n",
        "for epoch in range(max_epoch):\n",
        "    print(\"Epoch %d\" % (epoch + 1))\n",
        "    train_meta_loss = []\n",
        "    train_acc = []\n",
        "    # The \"step\" here is a meta-gradinet update step\n",
        "    for step in tqdm(range(max(1, len(train_loader) // meta_batch_size))):\n",
        "        x, train_iter = get_meta_batch(\n",
        "            meta_batch_size, k_shot, q_query, train_loader, train_iter\n",
        "        )\n",
        "        meta_loss, acc = Solver(\n",
        "            meta_model,\n",
        "            optimizer,\n",
        "            x,\n",
        "            n_way,\n",
        "            k_shot,\n",
        "            q_query,\n",
        "            loss_fn, \n",
        "            inner_train_step=train_inner_train_step\n",
        "        )\n",
        "        train_meta_loss.append(meta_loss.item())\n",
        "        train_acc.append(acc)\n",
        "    print(\"  Loss    : \", \"%.3f\" % (np.mean(train_meta_loss)), end=\"\\t\")\n",
        "    print(\"  Accuracy: \", \"%.3f %%\" % (np.mean(train_acc) * 100))\n",
        "\n",
        "    # See the validation accuracy after each epoch.\n",
        "    # Early stopping is welcomed to implement.\n",
        "    val_acc = []\n",
        "    for eval_step in tqdm(range(max(1, len(val_loader) // (eval_batches)))):\n",
        "        x, val_iter = get_meta_batch(\n",
        "            eval_batches, k_shot, q_query, val_loader, val_iter\n",
        "        )\n",
        "        # We update three inner steps when testing.\n",
        "        _, acc = Solver(\n",
        "            meta_model,\n",
        "            optimizer,\n",
        "            x,\n",
        "            n_way,\n",
        "            k_shot,\n",
        "            q_query,\n",
        "            loss_fn,\n",
        "            inner_train_step=val_inner_train_step,\n",
        "            train=False,\n",
        "        )\n",
        "        val_acc.append(acc)\n",
        "    if (np.mean(val_acc) * 100) > val_acc_best:\n",
        "      val_acc_best = (np.mean(val_acc) * 100)\n",
        "    print(\"bset is \",val_acc_best)\n",
        "    print(\"  Validation accuracy: \", \"%.3f %%\" % (np.mean(val_acc) * 100))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(meta_model.state_dict(), f\"/content/meta_best.ckpt\")"
      ],
      "metadata": {
        "id": "RpOEtrDoFxRj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Ew8-POf9sw"
      },
      "source": [
        "### Testing the result\n",
        "\n",
        "Since the testing data is sampled by TAs in advance, you should not change the code in `OmnigloTest` dataset, otherwise your score may not be correct on the Kaggle leaderboard.\n",
        "\n",
        "However, fell free to chagne the variable `inner_train_step` to set the training steps on the query set images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# test dataset\n",
        "class OmniglotTest(Dataset):\n",
        "    def __init__(self, test_dir):\n",
        "        self.test_dir = test_dir\n",
        "        self.n = 5\n",
        "\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        support_files = [\n",
        "            os.path.join(self.test_dir, \"support\", f\"{idx:>04}\", f\"image_{i}.png\")\n",
        "            for i in range(self.n)\n",
        "        ]\n",
        "        query_files = [\n",
        "            os.path.join(self.test_dir, \"query\", f\"{idx:>04}\", f\"image_{i}.png\")\n",
        "            for i in range(self.n)\n",
        "        ]\n",
        "\n",
        "        support_imgs = torch.stack(\n",
        "            [self.transform(Image.open(e)) for e in support_files]\n",
        "        )\n",
        "        query_imgs = torch.stack([self.transform(Image.open(e)) for e in query_files])\n",
        "\n",
        "        return support_imgs, query_imgs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(os.path.join(self.test_dir, \"support\")))"
      ],
      "metadata": {
        "id": "OKtTzxZeln5Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inner_train_step = 30 # you can change this\n",
        "\n",
        "test_batches = 20\n",
        "test_dataset = OmniglotTest(\"Omniglot-test\")\n",
        "test_loader = DataLoader(test_dataset, batch_size=test_batches, shuffle=False)\n",
        "\n",
        "output = []\n",
        "for _, batch in enumerate(tqdm(test_loader)):\n",
        "    support_set, query_set = batch\n",
        "    x = torch.cat([support_set, query_set], dim=1)\n",
        "    x = x.to(device)\n",
        "\n",
        "    labels = Solver(\n",
        "        meta_model,\n",
        "        optimizer,\n",
        "        x,\n",
        "        n_way,\n",
        "        k_shot,\n",
        "        q_query,\n",
        "        loss_fn,\n",
        "        inner_train_step=test_inner_train_step,\n",
        "        train=False,\n",
        "        return_labels=True,\n",
        "    )\n",
        "\n",
        "    output.extend(labels)\n",
        "\n",
        "# write to csv\n",
        "with open(\"output.csv\", \"w\") as f:\n",
        "    f.write(f\"id,class\\n\")\n",
        "    for i, label in enumerate(output):\n",
        "        f.write(f\"{i},{label}\\n\")"
      ],
      "metadata": {
        "id": "kTWHs1RThgGc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "47e3c1d635924a6f8bf421af30d3a750",
            "9e8e87a326ac42908b233a8d2a4e724f",
            "37c1a03113944a62b282c3ffb671668e",
            "b2549fe174eb41bc9a9c8287184503f0",
            "0a7d50239c0a4d12a8e5ac7a39f80196",
            "5aa6c9811d6245cb945ebc0bba9de744",
            "260d58e6e4bf488fb1f8e2a211504060",
            "e7fb7ffef19f4506a79ab4dbcc88b7a5",
            "405604dbf5eb4bf5a9fc492d9c1a3113",
            "63f22a0761f044b8baf229d10925a4af",
            "136db2916bb5419cb898ad9c02c02959"
          ]
        },
        "outputId": "877425a2-0e1d-44a2-afa9-383a176fe7af"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47e3c1d635924a6f8bf421af30d3a750"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the `output.csv` and submit to Kaggle!"
      ],
      "metadata": {
        "id": "yIfamxgMIaXw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtD8X3RLf-6w"
      },
      "source": [
        "## **Reference**\n",
        "1. Chelsea Finn, Pieter Abbeel, & Sergey Levine. (2017). [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.](https://arxiv.org/abs/1909.09157)\n",
        "1. Aniruddh Raghu, Maithra Raghu, Samy Bengio, & Oriol Vinyals. (2020). [Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML.](https://arxiv.org/abs/1909.09157)"
      ]
    }
  ]
}